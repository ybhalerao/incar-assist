{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec95bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install peft\n",
    "#!pip install sentencepiece\n",
    "#!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d39836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fdcec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed:int):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Collator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    max_len: int\n",
    "    text_field: str\n",
    "    def __call__(self, batch: List[Dict[str, str]]):\n",
    "        texts = [ex[self.text_field] for ex in batch]\n",
    "        toks = self.tokenizer(\n",
    "            texts,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = toks['input_ids']\n",
    "        attention_mask = toks['attention_mask']\n",
    "        # standard causal LM labels: next-token prediction (shifted inside loss)\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "\n",
    "def load_teacher(teacher_name: str, bits: int):\n",
    "    if bits == 16:\n",
    "        bnb = None\n",
    "    else:\n",
    "        bnb = BitsAndBytesConfig(\n",
    "            load_in_8bit=(bits==8),\n",
    "            load_in_4bit=(bits==4),\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "        )\n",
    "    teacher = AutoModelForCausalLM.from_pretrained(\n",
    "        teacher_name,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        quantization_config=bnb,\n",
    "    )\n",
    "    teacher.eval()\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    t_tok = AutoTokenizer.from_pretrained(teacher_name, use_fast=True)\n",
    "    if t_tok.pad_token is None:\n",
    "        t_tok.pad_token = t_tok.eos_token\n",
    "    return teacher, t_tok\n",
    "\n",
    "\n",
    "def load_student(student_name: str, bits: int, lora_cfg: LoraConfig):\n",
    "    if bits == 16:\n",
    "        quant_cfg = None\n",
    "    else:\n",
    "        quant_cfg = BitsAndBytesConfig(\n",
    "            load_in_8bit=(bits==8),\n",
    "            load_in_4bit=(bits==4),\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "        )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        student_name,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        quantization_config=quant_cfg,\n",
    "    )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(student_name, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    if bits in (4, 8):\n",
    "        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "    if lora_cfg is not None:\n",
    "        model = get_peft_model(model, lora_cfg)\n",
    "        model.print_trainable_parameters()\n",
    "    if hasattr(model, 'enable_input_require_grads'):\n",
    "        model.enable_input_require_grads()\n",
    "    if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "        model.gradient_checkpointing_enable()\n",
    "    return model, tok\n",
    "\n",
    "def kd_loss(student_logits, teacher_logits, labels, temperature=2.0, alpha_kl=0.9, alpha_ce=0.1):\n",
    "    \"\"\"Mixed loss: KL on softened distributions + optional CE on labels.\n",
    "    - student_logits, teacher_logits: [B, T, V]\n",
    "    - labels: [B, T] with -100 masking\n",
    "    \"\"\"\n",
    "    T = temperature\n",
    "    # Align lengths just in case\n",
    "    minT = min(student_logits.size(1), teacher_logits.size(1), labels.size(1))\n",
    "    s = student_logits[:, :minT, :]\n",
    "    t = teacher_logits[:, :minT, :]\n",
    "    y = labels[:, :minT]\n",
    "\n",
    "    s_log_probs = F.log_softmax(s / T, dim=-1)\n",
    "    t_probs = F.softmax(t / T, dim=-1)\n",
    "    kl = F.kl_div(s_log_probs, t_probs, reduction='none')  # [B, T, V]\n",
    "    # Mask KL by valid positions (labels != -100 OR attention positions)\n",
    "    valid = (y != -100).unsqueeze(-1).type_as(kl)\n",
    "    kl = (kl * valid).sum(-1)  # [B, T]\n",
    "    kl = kl.sum() / valid.sum().clamp(min=1.0)\n",
    "    kl = (T * T) * kl\n",
    "\n",
    "    ce = torch.tensor(0.0, device=s.device)\n",
    "    if alpha_ce > 0:\n",
    "        ce = F.cross_entropy(s.reshape(-1, s.size(-1)), y.reshape(-1), ignore_index=-100)\n",
    "\n",
    "    return alpha_kl * kl + alpha_ce * ce, {'kl': kl.detach(), 'ce': ce.detach()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d6185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e233cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__teacher_model = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "__teacher_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "__student_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "__dataset_name = \"json\"\n",
    "__data_files = '{\"train\": \"train.jsonl\", \"validation\": \"val.jsonl\"}'\n",
    "__text_field = \"text\"\n",
    "__output_dir = \"./kd_lora_tinymistral\"\n",
    "__max_seq_len = 1024\n",
    "__per_device_train_batch_size = 1\n",
    "__gradient_accumulation_steps = 16\n",
    "__learning_rate = 2e-4\n",
    "__num_train_epochs = 1\n",
    "__temperature = 2.0\n",
    "__alpha_kl = 0.9\n",
    "__alpha_ce = 0.1\n",
    "__lora_r = 16 \n",
    "__lora_alpha = 32 \n",
    "__lora_dropout = 0.05\n",
    "__target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "__teacher_bits = 8\n",
    "__student_bits = 4\n",
    "__bf16 = True\n",
    "__seed = 42\n",
    "__eval_steps = 0\n",
    "__weight_decay = 0.0\n",
    "\n",
    "__per_device_eval_batch_size=1\n",
    "__max_steps=-1\n",
    "__warmup_ratio=0.03\n",
    "__temperature=2.0\n",
    "__alpha_kl=0.9\n",
    "__alpha_ce=0.1\n",
    "\n",
    "__fp16=True\n",
    "__gradient_checkpointing=True\n",
    "\n",
    "__save_steps=500\n",
    "__logging_steps=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(__seed)\n",
    "os.makedirs(__output_dir, exist_ok=True)\n",
    "\n",
    "# LoRA config\n",
    "lora_cfg = LoraConfig(\n",
    "    r=__lora_r,\n",
    "    lora_alpha=__lora_alpha,\n",
    "    lora_dropout=__lora_dropout,\n",
    "    target_modules=__target_modules,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "\n",
    "# Load teacher & student\n",
    "print('Loading teacher...')\n",
    "teacher, teacher_tok = load_teacher(__teacher_model, __teacher_bits)\n",
    "print('Loading student...')\n",
    "student, tok = load_student(__student_model, __student_bits, lora_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d55d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "if __dataset_name == 'json':\n",
    "    if not __data_files:\n",
    "        raise ValueError('--data_files is required when dataset_name=json')\n",
    "    data_files = json.loads(__data_files)\n",
    "    ds = load_dataset('json', data_files=data_files)\n",
    "else:\n",
    "    if __data_files:\n",
    "        ds = load_dataset(__dataset_name, data_files=json.loads(__data_files))\n",
    "    else:\n",
    "        ds = load_dataset(__dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate = Collator(tokenizer=tok, max_len=__max_seq_len, text_field=__text_field)\n",
    "train_loader = DataLoader(ds['train'], batch_size=__per_device_train_batch_size, shuffle=True, collate_fn=collate)\n",
    "eval_loader = None\n",
    "if 'validation' in ds and __eval_steps != 0:\n",
    "    eval_loader = DataLoader(ds['validation'], batch_size=args.per_device_eval_batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "student.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd85acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optim & sched\n",
    "optim = torch.optim.AdamW(student.parameters(), lr=__learning_rate, weight_decay=__weight_decay)\n",
    "total_steps = __max_steps if __max_steps > 0 else int(len(train_loader) * __num_train_epochs // __gradient_accumulation_steps)\n",
    "warmup_steps = int(total_steps * __warmup_ratio)\n",
    "sched = get_cosine_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=__fp16)\n",
    "\n",
    "global_step = 0\n",
    "running = {'loss': 0.0, 'kl': 0.0, 'ce': 0.0}\n",
    "\n",
    "autocast_dtype = torch.bfloat16 if __bf16 else (torch.float16 if __fp16 else None)\n",
    "\n",
    "for epoch in range(__num_train_epochs if __max_steps <= 0 else 10_000_000):\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_out = teacher(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            t_logits = t_out.logits.detach()\n",
    "\n",
    "        if autocast_dtype is not None:\n",
    "            with torch.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', dtype=autocast_dtype):\n",
    "                s_out = student(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                s_logits = s_out.logits\n",
    "                loss, parts = kd_loss(s_logits, t_logits, labels, __temperature, __alpha_kl, __alpha_ce)\n",
    "        else:\n",
    "            s_out = student(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            s_logits = s_out.logits\n",
    "            loss, parts = kd_loss(s_logits, t_logits, labels, __temperature, __alpha_kl, __alpha_ce)\n",
    "\n",
    "        loss = loss / __gradient_accumulation_steps\n",
    "        if __fp16:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        running['loss'] += loss.item()\n",
    "        running['kl'] += parts['kl'].item() / __gradient_accumulation_steps\n",
    "        running['ce'] += parts['ce'].item() / __gradient_accumulation_steps\n",
    "\n",
    "        if (global_step + 1) % __gradient_accumulation_steps == 0:\n",
    "            if __fp16:\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optim.step()\n",
    "            sched.step()\n",
    "            student.zero_grad(set_to_none=True)\n",
    "\n",
    "            if (global_step // __gradient_accumulation_steps + 1) % __logging_steps == 0:\n",
    "                avg_loss = running['loss'] / __logging_steps\n",
    "                avg_kl = running['kl'] / __logging_steps\n",
    "                avg_ce = running['ce'] / __logging_steps\n",
    "                print(f\"step {global_step}: loss {avg_loss:.4f} | kl {avg_kl:.4f} | ce {avg_ce:.4f}\")\n",
    "                running = {'loss': 0.0, 'kl': 0.0, 'ce': 0.0}\n",
    "\n",
    "            if __save_steps and (global_step // __gradient_accumulation_steps + 1) % __save_steps == 0:\n",
    "                save_dir = os.path.join(__output_dir, f'step_{global_step}')\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                student.save_pretrained(save_dir)\n",
    "\n",
    "            if __eval_steps and (global_step // __gradient_accumulation_steps + 1) % __eval_steps == 0 and eval_loader is not None:\n",
    "                evaluate(student, eval_loader, device, autocast_dtype)\n",
    "\n",
    "        global_step += 1\n",
    "        if __max_steps > 0 and (global_step // __gradient_accumulation_steps) >= __max_steps:\n",
    "            break\n",
    "    if __max_steps > 0 and (global_step // __gradient_accumulation_steps) >= __max_steps:\n",
    "        break\n",
    "\n",
    "print('Saving final adapter...')\n",
    "student.save_pretrained(__output_dir)\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e954b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "__output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "!aws s3 cp --recursive ./kd_lora_tinymistral s3://data-daizika-com/incar_assist/model/kd_lora_tinymistral/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_loader, device, autocast_dtype):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc='Eval'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            if autocast_dtype is not None:\n",
    "                with torch.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', dtype=autocast_dtype):\n",
    "                    out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = out.loss\n",
    "            else:\n",
    "                out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = out.loss\n",
    "            losses.append(loss.item())\n",
    "    ppl = math.exp(sum(losses)/len(losses)) if losses else float('inf')\n",
    "    print(f\"Eval perplexity: {ppl:.2f}\")\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168cd3c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "__teacher_model  = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "__student_model  = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # base student *checkpoint*\n",
    "MODEL_DIR        = __output_dir\n",
    "\n",
    "# 1) Load the *base student tokenizer* (NOT from adapter dir)\n",
    "tok = AutoTokenizer.from_pretrained(__student_model, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# (Optional) 8-bit load to save RAM for inference\n",
    "bnb = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# 2) Load the *base student model*\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    __student_model,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb,                 # or remove if you want full-precision\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "\n",
    "# 3) Apply the LoRA adapter saved in MODEL_DIR\n",
    "model = PeftModel.from_pretrained(model, MODEL_DIR)\n",
    "model.eval()\n",
    "\n",
    "def infer(history: str, max_new=256):\n",
    "    inputs = tok(history, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(infer(\"<s>[INST] User: Find the nearest Toyota service center in Chicago. [/INST]\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc399fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "__output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd1bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
