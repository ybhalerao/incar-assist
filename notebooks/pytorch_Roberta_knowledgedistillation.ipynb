{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f513a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incar_assist_dataset():\n",
    "    input_file = \"s3://data-daizika-com/incar_assist/data/intent_classification/incar_assist_samples.csv\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    labels_df = df[['Label']]\n",
    "    input_file = \"s3://data-daizika-com/incar_assist/data/intent_classification/incar_assist_labels.csv\"\n",
    "    labels_df = pd.read_csv(input_file)\n",
    "    labels_dict = {rec['id']: rec['label'] for rec in labels_df.to_dict(orient=\"records\")}\n",
    "    df = df.set_index('Label').join(labels_df.set_index('label'), how=\"left\").reset_index()\n",
    "    df.columns = ['intent', 'text', 'label']\n",
    "    df_dict = df[['text', 'label']].to_dict(orient=\"records\")\n",
    "    return df_dict, labels_dict\n",
    "    \n",
    " # Example dataset: list of dicts with 'text' and 'label'\n",
    "#train_examples = [{\"text\":\"book me a flight\",\"label\":3}, ...]\n",
    "train_examples, labels_dict = get_incar_assist_dataset()\n",
    "\n",
    "def collate(batch):\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch])\n",
    "    enc = tok(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "    enc[\"labels\"] = labels\n",
    "    return enc\n",
    "\n",
    "NUM_LABELS = len(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_ckpt = \"roberta-large\"         # use your fine-tuned path here\n",
    "student_ckpt = \"roberta-base\"\n",
    "\n",
    "T = 4.0                                # temperature\n",
    "alpha = 0.9                            # weight for soft loss (teacher)\n",
    "lr = 2e-5\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(student_ckpt, use_fast=True)\n",
    "\n",
    "teacher = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_ckpt, num_labels=NUM_LABELS, output_hidden_states=True, output_attentions=True\n",
    ").to(device).eval()  # move to device!\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(student_ckpt)\n",
    "cfg.num_hidden_layers = 6\n",
    "cfg.num_labels = NUM_LABELS\n",
    "student = AutoModelForSequenceClassification.from_config(cfg).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(train_examples, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "opt = optim.AdamW(student.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    student.train()\n",
    "    for batch in dl:\n",
    "        batch = {k: v.cuda() if torch.cuda.is_available() else v for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            t_out = teacher(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            t_logits = t_out.logits\n",
    "\n",
    "        s_out = student(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\n",
    "        s_logits = s_out.logits\n",
    "\n",
    "        # Soft target loss (KLDiv between softened distributions)\n",
    "        log_p_s = F.log_softmax(s_logits / T, dim=-1)\n",
    "        p_t = F.softmax(t_logits / T, dim=-1)\n",
    "        kd_loss = F.kl_div(log_p_s, p_t, reduction=\"batchmean\") * (T * T)\n",
    "\n",
    "        # Hard label loss (optional but helpful if labels exist)\n",
    "        ce_loss = F.cross_entropy(s_logits, batch[\"labels\"])\n",
    "\n",
    "        loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "student.save_pretrained(\"roberta-student-distilled\")\n",
    "tok.save_pretrained(\"roberta-student-distilled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "!aws s3 cp --recursive ./roberta-student-distilled s3://data-daizika-com/incar_assist/model/roberta-student-distilled/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27884e",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 1️⃣ Load your saved model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../lambda/roberta-student-distilled\")\n",
    "#model_f16 = AutoModelForSequenceClassification.from_pretrained(\"../lambda/roberta-student-distilled\")\n",
    "#model_int8 = torch.quantization.quantize_dynamic(model_f16, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "model_int8 = AutoModelForSequenceClassification.from_pretrained(\"../lambda/roberta-student-distilled\")\n",
    "\n",
    "label_dict = {0: 'close door',\n",
    " 1: 'open door',\n",
    " 2: 'open window',\n",
    " 3: 'close window',\n",
    " 4: 'open bluetooth',\n",
    " 5: 'close bluetooth',\n",
    " 6: 'steering wheel',\n",
    " 7: 'camera'}\n",
    "\n",
    "# 2️⃣ Prepare your input text\n",
    "text = \"Please open the door\"\n",
    "\n",
    "# 3️⃣ Tokenize the text (convert to model inputs)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# 4️⃣ Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model_int8(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "print(f\"Predicted intent class: {labels_dict[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb59cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
