{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f513a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install bitsandbytes\n",
    "#!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f08d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch, torch.nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,               # teacher\n",
    "    AutoModelForSequenceClassification,  # student\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8c0ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEACHER_MODEL_NAME = \"openai/gpt-oss-20b\"  # or \"openai/gpt-oss-120b\"\n",
    "STUDENT_MODEL_NAME = \"distilroberta-base\"  # tiny options: \"prajjwal1/bert-tiny\", \"distilbert-base-uncased\"\n",
    "\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "LR = 3e-5\n",
    "EPOCHS = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c567c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incar_assist_dataset():\n",
    "    input_file = \"s3://data-daizika-com/incar_assist/data/intent_classification/incar_assist_samples.csv\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    labels_df = df[['Label']]\n",
    "    input_file = \"s3://data-daizika-com/incar_assist/data/intent_classification/incar_assist_labels.csv\"\n",
    "    labels_df = pd.read_csv(input_file)\n",
    "    labels2id_dict = {rec['label']: rec['id'] for rec in labels_df.to_dict(orient=\"records\")}\n",
    "    id2labels_dict = {rec['id']: rec['label'] for rec in labels_df.to_dict(orient=\"records\")}\n",
    "    df = df.set_index('Label').join(labels_df.set_index('label'), how=\"left\").reset_index()\n",
    "    df.columns = ['intent', 'text', 'label']\n",
    "    df_dict = df[['text', 'label']].to_dict(orient=\"records\")\n",
    "    return df_dict, labels2id_dict, id2labels_dict\n",
    "    \n",
    " # Example dataset: list of dicts with 'text' and 'label'\n",
    "#train_examples = [{\"text\":\"book me a flight\",\"label\":3}, ...]\n",
    "all_examples, label2id, id2label = get_incar_assist_dataset()\n",
    "all_texts = [rec['text'] for rec in all_examples]\n",
    "all_labels = [rec['label'] for rec in all_examples]\n",
    "verbalizers = {key:id2label[key].replace(\" \", \"-\") for key in id2label}\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(all_texts, all_labels, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c13ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Toy dataset (replace with your own)\n",
    "# -----------------------------\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int]):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"text\": self.texts[idx], \"label\": self.labels[idx]}\n",
    "\n",
    "train_ds = IntentDataset(train_texts, train_labels)\n",
    "val_ds   = IntentDataset(val_texts, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d42cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286e2e6361a94bbdb2f0545feeb264f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c451be98134492a955ddef0867b9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4212a006ee70460ea71674d3c92dae20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741dc28218f5428e9e088c26c23eda33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00000-of-00002.safetensors:   0%|          | 0.00/4.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8901161d52742feb6f0da1bf4c5030f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082447aa19e445ed930c7f1da8be75a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3) Load teacher & student\n",
    "# -----------------------------\n",
    "\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME)\n",
    "#teacher_model = AutoModelForCausalLM.from_pretrained(TEACHER_MODEL_NAME)\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    TEACHER_MODEL_NAME, num_labels=len(all_labels), output_hidden_states=True, output_attentions=True\n",
    ")\n",
    "teacher_model.to(DEVICE).eval()\n",
    "for p in teacher_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL_NAME, use_fast=True)\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    STUDENT_MODEL_NAME,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "student_model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f51c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c086977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Prompting & teacher soft labels\n",
    "# -----------------------------\n",
    "# Weâ€™ll treat distillation as next-token classification:\n",
    "#   prompt = \"Classify the driver command into one of: ...\\nCommand: {text}\\nLabel:\"\n",
    "# We read the logits for the *next token* and map them to our verbalizer token ids.\n",
    "# NOTE: This is an approximation. For better fidelity, score a short phrase (sum logprobs across tokens).\n",
    "\n",
    "label_first_token_ids: Dict[int, int] = {}\n",
    "for i, phrase in verbalizers.items():\n",
    "    # Get the first token id for the verbalizer under the *teacher tokenizer*.\n",
    "    tok = teacher_tokenizer(phrase, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    label_first_token_ids[i] = tok[\"input_ids\"][0, 0].item()\n",
    "\n",
    "def build_prompt(text: str) -> str:\n",
    "    choices = \", \".join([id2label[i] for i in range(len(id2label))])\n",
    "    return (\n",
    "        \"You are an expert intent classifier. \"\n",
    "        \"Respond with exactly one label token from the set below.\\n\"\n",
    "        f\"Valid labels: {choices}\\n\\n\"\n",
    "        f\"Command: {text}\\nLabel:\"\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def teacher_probs_for_batch(batch_texts: List[str]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns a float tensor of shape [B, num_labels] with teacher probability distribution.\n",
    "    \"\"\"\n",
    "    prompts = [build_prompt(t) for t in batch_texts]\n",
    "    enc = teacher_tokenizer(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Get logits for the next token after the prompt\n",
    "    out = teacher_model(**enc)\n",
    "    # For causal LMs, the next-token logits are the last position for each sequence.\n",
    "    next_token_logits = out.logits[:, -1, :]  # [B, vocab_size]\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Gather probability mass at our verbalizer token ids\n",
    "    indices = torch.tensor([label_first_token_ids[i] for i in range(len(id2label))], device=DEVICE)  # [num_labels]\n",
    "    gathered = probs.index_select(dim=1, index=indices)  # [B, num_labels] but not aligned; need gather per index\n",
    "    # index_select reorders columns to exactly those indices â€“ that's what we want\n",
    "    teacher_p = gathered  # [B, num_labels]\n",
    "    # Normalize again in case of any numerical drift (usually unnecessary)\n",
    "    teacher_p = teacher_p / teacher_p.sum(dim=1, keepdim=True)\n",
    "    return teacher_p\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Dataloaders\n",
    "# -----------------------------\n",
    "class Collator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.pad = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        texts = [f[\"text\"] for f in features]\n",
    "        labels = torch.tensor([f[\"label\"] for f in features], dtype=torch.long)\n",
    "        enc = self.pad(student_tokenizer(texts, truncation=True))\n",
    "        enc[\"labels\"] = labels\n",
    "        enc[\"texts\"] = texts  # keep raw for teacher\n",
    "        return enc\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=Collator(student_tokenizer))\n",
    "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, collate_fn=Collator(student_tokenizer))\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Distillation training loop\n",
    "# -----------------------------\n",
    "EPOCHS = 4\n",
    "LR = 5e-5\n",
    "WARMUP_STEPS = 0\n",
    "ALPHA_KD = 0.9       # weight on KD loss\n",
    "TEMPERATURE = 2.0    # temperature for KD\n",
    "ALPHA_CE = 1.0 - ALPHA_KD\n",
    "\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=LR)\n",
    "total_steps = EPOCHS * math.ceil(len(train_loader))\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, WARMUP_STEPS, total_steps)\n",
    "\n",
    "kl_div = nn.KLDivLoss(reduction=\"batchmean\")  # expects log-probs vs probs\n",
    "\n",
    "def kd_step(batch) -> Dict[str, float]:\n",
    "    student_model.train()\n",
    "    input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "    labels = batch[\"labels\"].to(DEVICE)\n",
    "    texts = batch[\"texts\"]\n",
    "\n",
    "    # 1) Teacher soft labels\n",
    "    with torch.no_grad():\n",
    "        t_probs = teacher_probs_for_batch(texts)  # [B, num_labels]\n",
    "        t_probs_T = F.softmax(torch.log(t_probs + 1e-12) / TEMPERATURE, dim=-1)  # optional re-temp of teacher\n",
    "\n",
    "    # 2) Student forward\n",
    "    out = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    s_logits = out.logits  # [B, num_labels]\n",
    "\n",
    "    # 3) KD loss (student log-softmax at T vs teacher probs at T)\n",
    "    s_log_probs_T = F.log_softmax(s_logits / TEMPERATURE, dim=-1)\n",
    "    loss_kd = kl_div(s_log_probs_T, t_probs_T) * (TEMPERATURE ** 2)\n",
    "\n",
    "    # 4) Optional CE on hard labels\n",
    "    loss_ce = F.cross_entropy(s_logits, labels)\n",
    "\n",
    "    loss = ALPHA_KD * loss_kd + ALPHA_CE * loss_ce\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = s_logits.argmax(dim=-1)\n",
    "        acc = (preds == labels).float().mean().item()\n",
    "\n",
    "    return {\"loss\": loss.item(), \"kd\": loss_kd.item(), \"ce\": loss_ce.item(), \"acc\": acc}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate() -> float:\n",
    "    student_model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    logs = []\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        info = kd_step(batch)\n",
    "        logs.append(info)\n",
    "        if step % 10 == 0 or step == len(train_loader):\n",
    "            print(f\"Epoch {epoch} | step {step}/{len(train_loader)} \"\n",
    "                  f\"loss={info['loss']:.4f} kd={info['kd']:.4f} ce={info['ce']:.4f} acc={info['acc']:.3f}\")\n",
    "    val_acc = evaluate()\n",
    "    print(f\"Epoch {epoch} done. Val acc: {val_acc:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Inference helper\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_intent(texts: List[str]) -> List[Dict]:\n",
    "    enc = student_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    logits = student_model(**enc).logits\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    preds = probs.argmax(dim=-1).tolist()\n",
    "    return [\n",
    "        {\n",
    "            \"text\": t,\n",
    "            \"pred_id\": p,\n",
    "            \"pred_label\": id2label[p],\n",
    "            \"probs\": {id2label[i]: float(probs[j, i]) for i in range(len(id2label))}\n",
    "        }\n",
    "        for j, (t, p) in enumerate(zip(texts, preds))\n",
    "    ]\n",
    "\n",
    "examples = [\n",
    "    \"please open bluetooth\",\n",
    "    \"switch off the bluetooth\",\n",
    "    \"show me the camera\",\n",
    "    \"close the door\",\n",
    "    \"open the window\"\n",
    "]\n",
    "print(predict_intent(examples))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
